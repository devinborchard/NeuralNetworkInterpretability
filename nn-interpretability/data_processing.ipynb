{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1wnufLVutb9MRyoKbcVuh7ytQitJ4P_1G","authorship_tag":"ABX9TyM+60BtQBwJGMrmpSw1olH3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"gJAqM-khbsd3","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1732733940522,"user_tz":300,"elapsed":369792,"user":{"displayName":"Devin Borchard","userId":"08620990790600685895"}},"outputId":"82568384-2a69-4547-9718-4ba543e91d26"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'that \\'s exactly how long the movie felt to me . there were n\\'t even nine laughs in nine months . it \\'s a terrible mess of a movie starring a terrible mess of a man , mr . hugh grant , a huge dork . it \\'s not the whole oral - sex / prostitution thing ( referring to grant , not me ) that bugs me , it \\'s the fact that grant is annoying . not just adam sandler - annoying , we \\'re talking jim carrey - annoying . since when do eye flutters and nervous smiles pass for acting ? but , on the other hand , since when do really bad slapstick ( a fistfight in the delivery room culminating in grant \\'s head in joan cusack \\'s lap -- a scene he paid $ 60 to have included in the movie ) and obscene double entendres ( robin williams , the obstetrician , tells grant \\'s pregnant girlfriend she has \" a big pussy , \" referring of course to the size of the cat hairs on her coat , but nonetheless , grant paid $ 60 to have the exchange included in the movie ) pass for comedy ? nine months is a predictable cookie - cutter movie with no originality in humor or plot . hugh grant plays a successful child psychiatrist . why a child psychologist ? so the scriptwriters could inject the following unfunny exchange : kid : my dad \\'s an asshole . grant ( flutters eyelashes , offers a nervous smile , then responds in his annoying english accent and i - think - i - actually - have- talent attitude ) : could you possibly elaborate on that ? kid : my dad \\'s a _ huge _ asshole . more like a hugh asshole , but that \\'s beside the point , which is : nine months includes too many needlessly stupid jokes that get laughs from the ten year olds in the audience while everyone else shakes his or her head in disbelief . so , anyway , grant finds out his girlfriend is pregnant and does his usual reaction ( fluttered eyelashes , nervous smiles ) . this paves the way for every possible pregnancy / child birth gag in the book , especially since grant \\'s equally annoying friend \\'s wife is also pregnant . the annoying friend is played by tom arnold , who provides most of the cacophonous slapstick , none of which is funny , such as a scene where arnold beats up a costumed \" arnie the dinosaur \" ( you draw your own parallels on that one ) in a toy store . the only interesting character in the movie is played by jeff goldblum , who should have hid himself away somewhere after the dreadful hideaway , as an artist with a fear of ( and simultaneous longing for ) commitment . not even robin williams , who plays a russian doctor who has recently decided to switch from veterinary medicine to obstetrics , has much humor . his is a one - joke character-- the old foreign - guy - who - mispronounces - english stereotype ( did someone say yakov smirnov ? that \\'s my favorite vodka , by the way ) , hence the line \" now it \\'s time to take a look at your volvo , \" another nasty but unamusing joke , except this one goes right over the ten year olds \\' heads , while the adults simultaneously groan . nine months is a complete failure , low on laughs and intelligence and high on loud , unfunny slapstick , failed jokes and other uninspired lunacy . hugh grant \\'s sunset boulevard arrest ( please , no caught - with - his - pants - down jokes ) may bring more people into the theaters , but they certainly wo n\\'t leave with a smile on their faces , not after 90 minutes of grant \\'s nervous smiles . everything in the movie is so forced , so unauthentic that anyone with an i . q . over 80 ( sorry , hugh ) will know they wasted their money on an unfulfilled desire . but at least they did n\\'t spend 60 bucks for it .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["import json\n","\n","movies_path = 'drive/MyDrive/google_colab_data/movies'\n","\n","def parse_data(file_path):\n","    data = []\n","    # Open the .jsonl file and read it line by line\n","    with open(file_path, 'r') as file:\n","        for line in file:\n","            # Parse each line as JSON and append it to the list\n","            annotation = json.loads(line)\n","            id = annotation[\"annotation_id\"]\n","            annotation[\"classification\"] = 1 if annotation['classification'] == \"POS\" else 0\n","\n","            with open(f\"{movies_path}/docs/{id}\", 'r') as file:\n","                content = file.read()\n","                annotation['content'] = content.replace('\\n', ' ')\n","                data.append(annotation)\n","    return data\n","\n","\n","# Specify the path to your JSON file\n","train_file_path = f'{movies_path}/train.jsonl'\n","test_file_path = f'{movies_path}/test.jsonl'\n","val_file_path = f'{movies_path}/val.jsonl'\n","\n","# Initialize an empty list to store the dictionaries\n","train_data = parse_data(train_file_path)\n","test_data = parse_data(test_file_path)\n","val_data = parse_data(val_file_path)\n","\n","train_data[7]['content']"]},{"cell_type":"code","source":["def print_example(data, index, print_content=True, print_classification=True, print_rationales=True ):\n","    print(f'Retrieving Training Example [{index}].................\\n')\n","    item = data[index]\n","    classification = item['classification']\n","    evidences = item['evidences']\n","    content = item['content']\n","    if print_content: print(f'Review content:\\n{content}\\n')\n","    if print_classification: print('----------------------------',\n","                                   '\\n| Sentiment class:',\n","                                   classification,\n","                                   (\"- NEG\" if not classification else \"- POS\"),\n","                                   '|', '\\n----------------------------')\n","    if print_rationales:\n","        print('\\nHuman rationales / Supporting Evidence:')\n","        for evidence in evidences:\n","            print('     - ', evidence[0])\n","\n","def get_content(data, index):\n","    item = data[index]\n","    content = item['content']\n","    return content\n","\n","def get_classes(data, index):\n","    item = data[index]\n","    classification = item['classification']\n","    return torch.tensor(classification)\n","\n","def get_annotations(data, index):\n","    item = data[index]\n","    content = item['evidences']\n","    annotations = [evidence for evidence in content]\n","    return annotations\n","\n","train_size = len(train_data)\n","test_size = len(test_data)\n","val_size = len(val_data)\n","\n","print(f'Dataset split: {train_size} training examples')\n","print(f'               {test_size} test examples\\n')\n","\n","print_example(train_data, 505)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aV9UQSh7cJSe","executionInfo":{"status":"ok","timestamp":1732733950019,"user_tz":300,"elapsed":138,"user":{"displayName":"Devin Borchard","userId":"08620990790600685895"}},"outputId":"a82a3ccf-53e0-4399-8a7c-29cfe39c2ad8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset split: 1600 training examples\n","               199 test examples\n","\n","Retrieving Training Example [505].................\n","\n","Review content:\n","well , what are you going to expect ? it 's a movie about a big snake that eats people . that 's what i should have been thinking when i viewed this film , because maybe then i would have enjoyed myself more . instead , i ended up wishing a giant snake would come along and eat me , too . anaconda is about a documentary film crew sailing down a south american river . led by anthropologists dr . steven cale ( eric stolz ) and terri flores ( jennifer lopez ) , the crew is attempting to locate a lost tribe of natives . along the way , they find poacher paul sarone ( jon voight ) , and become unwillingly embroiled in his quest to capture the elusive anaconda . to simply say that this is the world 's largest snake would n't be doing it justice , since the anaconda in this movie is at least two feet wide . if this is n't a good reason to avoid picking up hitchhikers , i do n't know what is . at the beginning of anaconda , we find that flores and cale have had some kind of relationship in the past , but that seems to be more or less over now . i expected that fact to play a key part somewhere down the line , but it ended up being a set - up for nothing . cale chokes on a deadly wasp ( do n't ask me how that happens - i do n't know ) and is put out of action , relegated to being the person whom the rest of the film crew must get back to civilization for medical help . this minimally helps to add a sense of urgency for the plot , since this goal now comes in direct conflict with sarone 's plan to capture the snake . however , this is negated by the fact that cale seems to get better by himself about halfway through the film , and in any event still results in zero payoff from the relationship angle . most bad characters are either annoying or stupid . in this movie , they 're both . since this description applies to all the characters except sarone , it 's hard to find someone to root for . you 're supposed to back the good guys , but you really end up cheering for sarone because he 's smarter than everyone else . or maybe he 's just not as dumb as everyone else . at times , i almost found myself rooting for the snake . there are no standout performances here . everyone seems to be reciting lines written for stock characters . even voight appears to be doing his best impression of christopher walken for some reason . no matter , as in most monster movies , the snake is supposed to be the real star anyway . in most of the scenes , the snake is computer generated , and the effects crew did a decent job of making it look real . however , the realism is thrown off by some pretty unrealistic occurrences . sometimes , for example , the snake just moves too fast . it catches a guy jumping off of a waterfall , for crying out loud . then there 's another scene where the snake eats one of the characters , and we see the snake 's skin drawn so tightly over its prey that we can see the victim 's pained expression from within the snake 's belly . absolutely ridiculous . a testimony to the film 's bad direction is the inclusion of a scene early on the in the film , where we get to see just how dangerous the mighty anaconda is . in a scene totally unrelated to anything else , we are witness to the big snake winning a showdown with a panther . the anaconda wraps itself around the powerful feline as if it were a stuffed animal and squeezes it so hard , one of the panther 's eyeballs pops out . eeeewwwww . above and beyond the sick factor , however , this scene surprised me because it actually showed the face of the snake before a quarter of the movie had even passed . in films like these , a sense of mystery surrounding the monster must be maintained . if the characters are reacting to something they fear more than see , we as an audience must experience that feeling along with them . to show us the monster early on is to let us in on something the characters do n't know about , and therefore allow us to get used to the danger before the pivotal moment when man and beast have their climactic showdown . in a case like that , the showdown just ends up being a letdown . when i saw the snake for the first time , i decided to give the film the benefit of the doubt and assume that the snake i saw was n't the real danger . this one was just a decoy , and there was actually a bigger snake waiting to make its appearance just when everyone thought they were safe . no such luck . okay , maybe anaconda is actually a decoy , and there 's really a better movie waiting to make its debut .\n","\n","---------------------------- \n","| Sentiment class: 0 - NEG | \n","----------------------------\n","\n","Human rationales / Supporting Evidence:\n","     -  {'docid': 'negR_505.txt', 'end_sentence': 27, 'end_token': 538, 'start_sentence': 26, 'start_token': 528, 'text': 'the realism is thrown off by some pretty unrealistic occurrences'}\n","     -  {'docid': 'negR_505.txt', 'end_sentence': 15, 'end_token': 354, 'start_sentence': 14, 'start_token': 349, 'text': 'still results in zero payoff'}\n","     -  {'docid': 'negR_505.txt', 'end_sentence': 4, 'end_token': 60, 'start_sentence': 3, 'start_token': 47, 'text': 'i ended up wishing a giant snake would come along and eat me'}\n","     -  {'docid': 'negR_505.txt', 'end_sentence': 31, 'end_token': 611, 'start_sentence': 30, 'start_token': 609, 'text': 'absolutely ridiculous'}\n","     -  {'docid': 'negR_505.txt', 'end_sentence': 22, 'end_token': 451, 'start_sentence': 21, 'start_token': 446, 'text': 'there are no standout performances'}\n","     -  {'docid': 'negR_505.txt', 'end_sentence': 17, 'end_token': 375, 'start_sentence': 15, 'start_token': 360, 'text': \"bad characters are either annoying or stupid . in this movie , they 're both\"}\n","     -  {'docid': 'negR_505.txt', 'end_sentence': 40, 'end_token': 832, 'start_sentence': 39, 'start_token': 826, 'text': 'just ends up being a letdown'}\n","     -  {'docid': 'negR_505.txt', 'end_sentence': 36, 'end_token': 707, 'start_sentence': 34, 'start_token': 699, 'text': 'eeeewwwww . above and beyond the sick factor'}\n","     -  {'docid': 'negR_505.txt', 'end_sentence': 43, 'end_token': 897, 'start_sentence': 42, 'start_token': 894, 'text': 'no such luck'}\n"]}]},{"cell_type":"code","source":["def encode_annotations(reviews, evidences):\n","  encoded_annotations = []\n","  for index in range(len(reviews)):\n","    review = reviews[index]\n","    evidence = evidences[index]\n","    annotation = np.zeros(max_length)\n","    for e in evidence:\n","      e = e[0]\n","      start = e['start_token']\n","      end = e['end_token']\n","      if(start < max_length and end < max_length):\n","        annotation[start:end] = 1\n","\n","    encoded_annotations.append(annotation)\n","  return torch.tensor(encoded_annotations)"],"metadata":{"id":"cinCMffqCi9k","executionInfo":{"status":"ok","timestamp":1732734761490,"user_tz":300,"elapsed":137,"user":{"displayName":"Devin Borchard","userId":"08620990790600685895"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from typing import List, Dict, Union\n","from transformers import BertTokenizerFast\n","\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","max_length = 512\n","# With it being easy to generate batches of tokenized texts, it's actually easier\n","# not to do the tokenization beforehand, and just store texts\n","# It's a little bit slow though, so if you found this to be bottleneck\n","# you'd want to pre-tokenize everything and then batch/pad as necessary\n","class SST2TransformerDataset(Dataset):\n","  def __init__(self,\n","               labels=None,\n","               texts=None,\n","               evidences=None\n","               ):\n","\n","    self.y = torch.tensor(labels,dtype=torch.int64)\n","    self.texts = texts\n","    self.evidences = evidences\n","\n","  def __len__(self):\n","    return self.y.shape[0]\n","\n","  def __getitem__(self, idx):\n","    rdict = {\n","      'y': self.y[idx],\n","      'text': self.texts[idx],\n","      'evidences': self.evidences[idx]\n","    }\n","    return rdict\n","\n","\n","def SST2_transformer_collate(batch:List[Dict[str, Union[torch.Tensor,str]]]):\n","  # print(\"BATCH: \", batch)\n","  y_batch = torch.tensor([example['y'] for example in batch])\n","  evidences = [example['evidences'].numpy() for example in batch]\n","\n","  # We'll just reuse the tokenizer we created earlier, since it doesn't change\n","  tokenized_batch = tokenizer.batch_encode_plus([example['text'] for example in batch],\n","                                                return_tensors='pt',\n","                                                padding=True,\n","                                                max_length=max_length,\n","                                                truncation=True)\n","\n","  return {\n","      'y':y_batch,\n","      'input_ids':tokenized_batch['input_ids'],\n","      'attention_mask':tokenized_batch['attention_mask'],\n","      'evidences':torch.tensor(evidences)\n","  }\n","\n","reviews = [get_content(train_data, i) for i in range(train_size)]\n","classes = [float(get_classes(train_data, i)) for i in range(train_size)]\n","evidences = [get_annotations(train_data, i) for i in range(train_size)]\n","encoded_evidences = encode_annotations(reviews, evidences)\n","\n","reviews_test = [get_content(test_data, i) for i in range(test_size)]\n","classes_test = [float(get_classes(test_data, i)) for i in range(test_size)]\n","evidences_test = [get_annotations(test_data, i) for i in range(test_size)]\n","encoded_evidences_test = encode_annotations(reviews_test, evidences_test)\n","\n","reviews_val = [get_content(val_data, i) for i in range(val_size)]\n","classes_val = [float(get_classes(val_data, i)) for i in range(val_size)]\n","evidences_val = [get_annotations(val_data, i) for i in range(val_size)]\n","encoded_evidences_val = encode_annotations(reviews_val, evidences_val)\n","\n","\n","\n","\n","batch_size = 10\n","train_dataset = SST2TransformerDataset(classes, reviews, encoded_evidences)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn = SST2_transformer_collate, shuffle=True)\n","\n","dev_dataset = SST2TransformerDataset(classes_test, reviews_test, encoded_evidences_test)\n","dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn = SST2_transformer_collate, shuffle=False)\n","\n","val_dataset = SST2TransformerDataset(classes_val, reviews_val, encoded_evidences_val)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn = SST2_transformer_collate, shuffle=False)"],"metadata":{"id":"wHKInHxcg6_U","executionInfo":{"status":"ok","timestamp":1732736544598,"user_tz":300,"elapsed":568,"user":{"displayName":"Devin Borchard","userId":"08620990790600685895"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["train_data_path = 'drive/MyDrive/google_colab_data/train_data_loader.pt'\n","test_data_path = 'drive/MyDrive/google_colab_data/test_data_loader.pt'\n","val_data_path = 'drive/MyDrive/google_colab_data/val_data_loader.pt'\n","\n","def save_dataloader(dataloader_x, dataloader_y, encoded_evidences, dataloader, path, shuffle):\n","  !mkdir -p path\n","  # Save the data (input, Y_star) and DataLoader parameters\n","  save_data = {\n","      'input': dataloader_x,\n","      'Y_star': dataloader_y,\n","      'evidence': encoded_evidences,\n","      'dataloader_params': {\n","          'batch_size': dataloader.batch_size,\n","          'shuffle': shuffle,  # Keep track of shuffle manually\n","          'num_workers': dataloader.num_workers,\n","      }\n","  }\n","  # Save the data\n","  torch.save(save_data, path)\n","\n","save_dataloader(reviews, classes, encoded_evidences, train_dataloader, train_data_path, True)\n","save_dataloader(reviews_test, classes_test, encoded_evidences_test, dev_dataloader, test_data_path, False)\n","save_dataloader(reviews_val, classes_val, encoded_evidences_val, val_dataloader, val_data_path, False)"],"metadata":{"id":"XKfbKHK0pgLs","executionInfo":{"status":"ok","timestamp":1732736547766,"user_tz":300,"elapsed":454,"user":{"displayName":"Devin Borchard","userId":"08620990790600685895"}}},"execution_count":25,"outputs":[]}]}